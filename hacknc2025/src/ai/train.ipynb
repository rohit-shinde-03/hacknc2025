{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69405207",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train the GPT2 model on NES-MDB MIDI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54c9eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_42908\\1391862299.py:23: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"): np.object = object\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_42908\\1391862299.py\", line 29, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]\n",
      "Torch: 2.4.0+cu121 | CUDA build: 12.1 | cuda available: True\n",
      "GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.44.2\n",
      "Datasets: 2.21.0\n",
      "miditok: 3.0.6\n",
      "miditoolkit: 0.1.16\n",
      "pretty_midi: 0.2.9\n",
      "numpy: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # NES-MDB Chiptune Transformer — Minimal, Robust Notebook (2025)\n",
    "# - Tokenize raw MIDIs directly (monophonic skyline) with **MIDILike** (no durations)\n",
    "# - Build dataset with short-clip support and fallback seq_len\n",
    "# - Train a small GPT-like model (Transformers)\n",
    "# - Generate continuation + try to write MIDI\n",
    "#\n",
    "# Works with:\n",
    "#   torch (CUDA if available), transformers 4.4x, datasets 2.2x,\n",
    "#   miditok 3.0.6, miditoolkit 0.1.16, pretty_midi 0.2.9, numpy 2.x\n",
    "\n",
    "# =========================\n",
    "# 0) Compatibility & env check\n",
    "# =========================\n",
    "# %%\n",
    "import sys, os, json, random, numpy as np\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "\n",
    "# NumPy 2.x removed np.int / np.bool etc; some libs still reference them\n",
    "if not hasattr(np, \"int\"):    np.int = int\n",
    "if not hasattr(np, \"bool\"):   np.bool = bool\n",
    "if not hasattr(np, \"float\"):  np.float = float\n",
    "if not hasattr(np, \"object\"): np.object = object\n",
    "\n",
    "def pkgver(name: str) -> str:\n",
    "    try: return version(name)\n",
    "    except PackageNotFoundError: return \"not-found\"\n",
    "\n",
    "import torch\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA build:\", torch.version.cuda, \"| cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "import transformers, datasets\n",
    "print(\"Transformers:\", pkgver(\"transformers\"))\n",
    "print(\"Datasets:\",    pkgver(\"datasets\"))\n",
    "print(\"miditok:\",     pkgver(\"miditok\"))\n",
    "print(\"miditoolkit:\", pkgver(\"miditoolkit\"))\n",
    "print(\"pretty_midi:\", pkgver(\"pretty_midi\"))\n",
    "print(\"numpy:\",       np.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\rohit\\Downloads\\hacknc2025\\hacknc2025\\src\\ai\\data\\nesmdb_midi exists: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 1) Paths & knobs\n",
    "# =========================\n",
    "# %%\n",
    "from pathlib import Path\n",
    "\n",
    "REPO      = Path.cwd()\n",
    "DATA_DIR  = REPO / \"data\" / \"nesmdb_midi\"   # <--- put your raw MIDIs here\n",
    "WORK      = REPO / \"nes_transformer\"\n",
    "\n",
    "TOK_DIR   = WORK / \"tokens\"       # token JSONs { \"ids\": [...] }\n",
    "RUN_DIR   = WORK / \"hf_runs\"      # HF checkpoints / logs\n",
    "SAMPLES   = WORK / \"samples\"      # generated MIDIs\n",
    "for p in [WORK, TOK_DIR, RUN_DIR, SAMPLES]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR.resolve(), \"exists:\", DATA_DIR.exists())\n",
    "\n",
    "# --- tokenization speed/quality knobs ---\n",
    "SUBSET_N   = 2000      # process only this many raw MIDIs now (None = all)  << adjust for time\n",
    "SEED       = 42\n",
    "LO_PITCH   = 48        # C3\n",
    "HI_PITCH   = 96        # C7\n",
    "MAX_TICKS  = None      # e.g. 20000 to crop early for speed; None = full file (more robust)\n",
    "USE_THREADS= False     # start serial for stability; flip to True for speed once it works\n",
    "MAX_WORKERS= max(4, os.cpu_count() or 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a98e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: MIDILike | vocab_size: 338\n",
      "Raw MIDIs found: 10556\n",
      "Using subset: 1000 / 10556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing (serial):   0%|          | 0/1000 [00:00<?, ?it/s]C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_42908\\2574420059.py:93: UserWarning: You are using a depreciated `miditoolkit.MidiFile` object. MidiTokis now (>v3.0.0) using symusic.Score as MIDI backend. Your file willbe converted on the fly, however please consider using symusic.\n",
      "  toks = tokenizer.tokenize(one) if hasattr(tokenizer, \"tokenize\") else tokenizer(one)\n",
      "Tokenizing (serial): 100%|██████████| 1000/1000 [02:25<00:00,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token files written: 0 → c:\\Users\\rohit\\Downloads\\hacknc2025\\hacknc2025\\src\\ai\\nes_transformer\\tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 2) Tokenize directly from RAW (MIDILike, skyline to 1-voice)\n",
    "# =========================\n",
    "# %%\n",
    "# Patch miditoolkit.Note with a .duration property (needed by miditok's converter under the hood)\n",
    "import miditoolkit\n",
    "try:\n",
    "    from miditoolkit.midi.containers import Note as MTKNote\n",
    "except Exception:\n",
    "    from miditoolkit import Note as MTKNote\n",
    "if not hasattr(MTKNote, \"duration\"):\n",
    "    MTKNote.duration = property(lambda self: self.end - self.start)\n",
    "\n",
    "from miditoolkit import MidiFile, Instrument, Note\n",
    "from miditok import MIDILike, TokenizerConfig, TokSequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Minimal tokenizer config (avoid durations/tempos/rets complexity)\n",
    "tok_config = TokenizerConfig(\n",
    "    beat_res={(0,0):4},     # 16th grid for positions\n",
    "    use_chords=False,\n",
    "    use_rests=False,\n",
    "    use_tempos=False,\n",
    "    use_time_signatures=False,\n",
    "    use_programs=False\n",
    ")\n",
    "tokenizer = MIDILike(tok_config)\n",
    "print(\"Tokenizer: MIDILike | vocab_size:\", tokenizer.vocab_size)\n",
    "\n",
    "# Skyline melody extractor (ticks): always monophonic\n",
    "def skyline_ticks(notes, min_dur=1):\n",
    "    if not notes: return []\n",
    "    times = sorted({n.start for n in notes} | {n.end for n in notes})\n",
    "    out = []\n",
    "    cur_pitch, cur_start = None, None\n",
    "    for t in times:\n",
    "        active = [n for n in notes if n.start <= t < n.end]\n",
    "        if active:\n",
    "            pitch = max(active, key=lambda n: n.pitch).pitch\n",
    "            if pitch != cur_pitch:\n",
    "                if cur_pitch is not None and (t - cur_start) >= min_dur:\n",
    "                    out.append(Note(velocity=90, pitch=cur_pitch, start=cur_start, end=t))\n",
    "                cur_pitch, cur_start = pitch, t\n",
    "        else:\n",
    "            if cur_pitch is not None and (t - cur_start) >= min_dur:\n",
    "                out.append(Note(velocity=90, pitch=cur_pitch, start=cur_start, end=t))\n",
    "            cur_pitch, cur_start = None, None\n",
    "    return out\n",
    "\n",
    "# Gather raw files (+ optional subset)\n",
    "raw_all = sorted([p for ext in (\"*.mid\",\"*.midi\",\"*.MID\",\"*.MIDI\") for p in DATA_DIR.rglob(ext)])\n",
    "print(\"Raw MIDIs found:\", len(raw_all))\n",
    "if SUBSET_N is not None and SUBSET_N < len(raw_all):\n",
    "    random.seed(SEED)\n",
    "    raw = sorted(random.sample(raw_all, SUBSET_N))\n",
    "    print(f\"Using subset: {len(raw)} / {len(raw_all)}\")\n",
    "else:\n",
    "    raw = raw_all\n",
    "    print(\"Using ALL raw files\")\n",
    "\n",
    "def tokenize_one(path: Path) -> Path | None:\n",
    "    out = TOK_DIR / (path.stem + \".json\")\n",
    "    if out.exists():\n",
    "        return out\n",
    "    try:\n",
    "        mf = MidiFile(str(path))\n",
    "        # collect notes (non-drum, in range), optional crop for speed\n",
    "        cand = []\n",
    "        for inst in mf.instruments:\n",
    "            if inst.is_drum or not inst.notes:\n",
    "                continue\n",
    "            notes = inst.notes\n",
    "            if MAX_TICKS is not None:\n",
    "                notes = [n for n in notes if n.start < MAX_TICKS]\n",
    "            notes = [n for n in notes if LO_PITCH <= n.pitch <= HI_PITCH]\n",
    "            cand.extend(notes)\n",
    "        if len(cand) < 4:\n",
    "            return None\n",
    "\n",
    "        mono = skyline_ticks(cand, min_dur=1)\n",
    "        if len(mono) < 4:\n",
    "            return None\n",
    "\n",
    "        # Build tiny 1-track MIDI in memory (keeps original grid)\n",
    "        one = MidiFile(ticks_per_beat=mf.ticks_per_beat)\n",
    "        one.tempo_changes = mf.tempo_changes\n",
    "        one.time_signature_changes = mf.time_signature_changes\n",
    "        inst = Instrument(program=80, is_drum=False, name=\"lead\")\n",
    "        inst.notes = mono\n",
    "        one.instruments = [inst]\n",
    "\n",
    "        # Tokenize (MIDILike)\n",
    "        toks = tokenizer.tokenize(one) if hasattr(tokenizer, \"tokenize\") else tokenizer(one)\n",
    "        ids  = toks.ids if hasattr(toks, \"ids\") else toks\n",
    "        if not ids:\n",
    "            return None\n",
    "\n",
    "        out.write_text(json.dumps({\"ids\": ids}))\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "token_files = []\n",
    "if USE_THREADS:\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futures = [ex.submit(tokenize_one, p) for p in raw]\n",
    "        for f in tqdm(as_completed(futures), total=len(futures), desc=\"Tokenizing (threads)\"):\n",
    "            res = f.result()\n",
    "            if res is not None:\n",
    "                token_files.append(res)\n",
    "else:\n",
    "    for p in tqdm(raw, desc=\"Tokenizing (serial)\"):\n",
    "        res = tokenize_one(p)\n",
    "        if res is not None:\n",
    "            token_files.append(res)\n",
    "\n",
    "print(\"Token files written:\", len(token_files), \"→\", TOK_DIR)\n",
    "\n",
    "# Simple peek\n",
    "if token_files:\n",
    "    sample_ids = json.loads(token_files[0].read_text())[\"ids\"]\n",
    "    print(\"Sample token length:\", len(sample_ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b0403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\rohit\\Downloads\\hacknc2025\\hacknc2025\\src\\ai\\data\\nesmdb_midi | exists: True\n",
      "Raw files: 10556\n",
      "Using subset: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing (simple): 100%|██████████| 1000/1000 [00:00<00:00, 2477.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token files written: 940 → c:\\Users\\rohit\\Downloads\\hacknc2025\\hacknc2025\\src\\ai\\nes_transformer\\tokens_simple\n",
      "VOCAB_SIZE: 131 | PAD/REST/HOLD ids: 0 1 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 3) Simple 16th-grid tokenizer (no MidiTok)\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "import json, os, random, math\n",
    "\n",
    "from miditoolkit import MidiFile, Instrument, Note\n",
    "\n",
    "# ---- Paths (redefine if kernel was restarted) ----\n",
    "REPO      = Path.cwd()\n",
    "DATA_DIR  = REPO / \"data\" / \"nesmdb_midi\"   # raw MIDIs here\n",
    "WORK      = REPO / \"nes_transformer\"\n",
    "TOK_DIR   = WORK / \"tokens_simple\"\n",
    "SAMPLES   = WORK / \"samples\"\n",
    "for p in [WORK, TOK_DIR, SAMPLES]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR.resolve(), \"| exists:\", DATA_DIR.exists())\n",
    "\n",
    "# ---- Token vocabulary: PAD=0, REST=1, HOLD=2, P0..P127=3..130 ----\n",
    "PAD_ID  = 0\n",
    "REST_ID = 1\n",
    "HOLD_ID = 2\n",
    "PITCH_BASE = 3  # P0 maps to 3, P127 maps to 130\n",
    "VOCAB_SIZE = PITCH_BASE + 128\n",
    "\n",
    "def pitch_to_id(p: int) -> int:\n",
    "    p = max(0, min(127, int(p)))\n",
    "    return PITCH_BASE + p\n",
    "\n",
    "def id_to_pitch(i: int) -> int | None:\n",
    "    if i >= PITCH_BASE and i < PITCH_BASE + 128:\n",
    "        return i - PITCH_BASE\n",
    "    return None  # REST/HOLD/PAD\n",
    "\n",
    "# ---- Monophonic skyline over ticks ----\n",
    "def skyline_ticks(notes, min_dur_ticks: int = 1):\n",
    "    if not notes:\n",
    "        return []\n",
    "    times = sorted({n.start for n in notes} | {n.end for n in notes})\n",
    "    out = []\n",
    "    cur_pitch, cur_start = None, None\n",
    "    for t in times:\n",
    "        active = [n for n in notes if n.start <= t < n.end]\n",
    "        if active:\n",
    "            pitch = max(active, key=lambda n: n.pitch).pitch\n",
    "            if pitch != cur_pitch:\n",
    "                if cur_pitch is not None and (t - cur_start) >= min_dur_ticks:\n",
    "                    out.append((cur_pitch, cur_start, t))\n",
    "                cur_pitch, cur_start = pitch, t\n",
    "        else:\n",
    "            if cur_pitch is not None and (t - cur_start) >= min_dur_ticks:\n",
    "                out.append((cur_pitch, cur_start, t))\n",
    "            cur_pitch, cur_start = None, None\n",
    "    return out\n",
    "\n",
    "# ---- Quantize monophonic notes to a 16th grid ----\n",
    "def quantize_to_grid_16th(mf: MidiFile, mono_notes, max_steps: int | None = None):\n",
    "    tpq = max(1, int(mf.ticks_per_beat))\n",
    "    ticks_per_step = max(1, tpq // 4)  # 16th = TPQ/4\n",
    "    if not mono_notes:\n",
    "        return []\n",
    "\n",
    "    max_tick = 0\n",
    "    for pitch, s, e in mono_notes:\n",
    "        max_tick = max(max_tick, e)\n",
    "\n",
    "    total_steps = math.ceil(max_tick / ticks_per_step)\n",
    "    if max_steps is not None:\n",
    "        total_steps = min(total_steps, max_steps)\n",
    "\n",
    "    seq = [REST_ID] * max(1, total_steps)\n",
    "\n",
    "    cur_idx = 0\n",
    "    for pitch, s, e in mono_notes:\n",
    "        start_idx = int(round(s / ticks_per_step))\n",
    "        end_idx   = int(max(start_idx + 1, round(e / ticks_per_step)))\n",
    "        if max_steps is not None:\n",
    "            start_idx = min(start_idx, max_steps - 1)\n",
    "            end_idx   = min(end_idx,   max_steps)\n",
    "\n",
    "        # fill leading rest if any\n",
    "        while cur_idx < start_idx and cur_idx < len(seq):\n",
    "            seq[cur_idx] = REST_ID\n",
    "            cur_idx += 1\n",
    "\n",
    "        if start_idx < len(seq):\n",
    "            seq[start_idx] = pitch_to_id(pitch)\n",
    "            cur_idx = start_idx + 1\n",
    "\n",
    "        # fill holds\n",
    "        while cur_idx < end_idx and cur_idx < len(seq):\n",
    "            seq[cur_idx] = HOLD_ID\n",
    "            cur_idx += 1\n",
    "\n",
    "    return seq\n",
    "\n",
    "# ---- File → tokens pipeline ----\n",
    "def midi_to_token_ids(path: Path, lo_pitch: int | None = None, hi_pitch: int | None = None,\n",
    "                      crop_ticks: int | None = None, max_steps: int | None = 2048):\n",
    "    try:\n",
    "        mf = MidiFile(str(path))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    # collect non-drum notes (optionally filter pitch / crop)\n",
    "    notes = []\n",
    "    for inst in mf.instruments:\n",
    "        if inst.is_drum or not inst.notes:\n",
    "            continue\n",
    "        ns = inst.notes\n",
    "        if crop_ticks is not None:\n",
    "            ns = [n for n in ns if n.start < crop_ticks]\n",
    "        if lo_pitch is not None and hi_pitch is not None:\n",
    "            ns = [n for n in ns if lo_pitch <= n.pitch <= hi_pitch]\n",
    "        notes.extend(ns)\n",
    "\n",
    "    if not notes:\n",
    "        return None\n",
    "\n",
    "    mono = skyline_ticks(notes, min_dur_ticks=1)\n",
    "    if not mono:\n",
    "        return None\n",
    "\n",
    "    ids = quantize_to_grid_16th(mf, mono, max_steps=max_steps)\n",
    "    # keep only reasonably sized sequences\n",
    "    return ids if len(ids) >= 16 else None\n",
    "\n",
    "# ---- Batch tokenization ----\n",
    "SUBSET_N   = 2000          # start small to confirm; bump to 1500–3000 when it works\n",
    "SEED       = 42\n",
    "LO_PITCH   = None         # None means keep all pitches (widest, safest)\n",
    "HI_PITCH   = None\n",
    "CROP_TICKS = None         # set e.g. 20000 to speed up\n",
    "MAX_STEPS  = 1024         # cap per piece\n",
    "\n",
    "raw_all = sorted([p for ext in (\"*.mid\",\"*.midi\",\"*.MID\",\"*.MIDI\") for p in DATA_DIR.rglob(ext)])\n",
    "print(\"Raw files:\", len(raw_all))\n",
    "random.seed(SEED)\n",
    "raw = sorted(random.sample(raw_all, min(SUBSET_N, len(raw_all))))\n",
    "print(\"Using subset:\", len(raw))\n",
    "\n",
    "written = 0\n",
    "for p in tqdm(raw, desc=\"Tokenizing (simple)\"):\n",
    "    out = TOK_DIR / f\"{p.stem}.json\"\n",
    "    if out.exists():\n",
    "        written += 1\n",
    "        continue\n",
    "    ids = midi_to_token_ids(p, lo_pitch=LO_PITCH, hi_pitch=HI_PITCH,\n",
    "                            crop_ticks=CROP_TICKS, max_steps=MAX_STEPS)\n",
    "    if ids is None:\n",
    "        continue\n",
    "    out.write_text(json.dumps({\"ids\": ids}))\n",
    "    written += 1\n",
    "\n",
    "print(\"Token files written:\", written, \"→\", TOK_DIR)\n",
    "print(\"VOCAB_SIZE:\", VOCAB_SIZE, \"| PAD/REST/HOLD ids:\", PAD_ID, REST_ID, HOLD_ID)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef835d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ_LEN=512 -> sequences: 952\n",
      "USING SEQ_LEN=512 | train=904 | test=48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 904\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 4) Build dataset (keeps short clips; 512→256→128 fallback)\n",
    "# =========================\n",
    "from datasets import Dataset\n",
    "import json, random\n",
    "\n",
    "def build_ds(token_dir: Path, seq_len: int, keep_short_min: int = 8, step_frac: float = 0.5):\n",
    "    files = sorted(token_dir.glob(\"*.json\"))\n",
    "    sequences = []\n",
    "    step = max(1, int(seq_len * step_frac))\n",
    "    for p in files:\n",
    "        try:\n",
    "            ids = json.loads(p.read_text())[\"ids\"]\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not ids:\n",
    "            continue\n",
    "        if len(ids) <= seq_len:\n",
    "            if len(ids) >= keep_short_min:\n",
    "                sequences.append({\"input_ids\": ids, \"labels\": ids.copy()})\n",
    "            continue\n",
    "        # sliding window\n",
    "        for i in range(0, len(ids) - seq_len + 1, step):\n",
    "            seq = ids[i:i+seq_len]\n",
    "            sequences.append({\"input_ids\": seq, \"labels\": seq.copy()})\n",
    "    return sequences\n",
    "\n",
    "SEQ_TRY = [512, 256, 128]\n",
    "final_sequences, final_len = None, None\n",
    "for L in SEQ_TRY:\n",
    "    seqs = build_ds(TOK_DIR, seq_len=L, keep_short_min=8, step_frac=0.5)\n",
    "    print(f\"SEQ_LEN={L} -> sequences: {len(seqs)}\")\n",
    "    if seqs:\n",
    "        final_sequences, final_len = seqs, L\n",
    "        break\n",
    "\n",
    "if final_sequences is None:\n",
    "    raise RuntimeError(\"No sequences produced. Increase SUBSET_N, set CROP_TICKS=None, or lower keep_short_min to 4, then re-run.\")\n",
    "\n",
    "random.shuffle(final_sequences)\n",
    "ds = Dataset.from_list(final_sequences).train_test_split(test_size=0.05, seed=42)\n",
    "print(f\"USING SEQ_LEN={final_len} | train={len(ds['train'])} | test={len(ds['test'])}\")\n",
    "ds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9569fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "  0%|          | 0/339 [00:00<?, ?it/s]c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:544: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      " 15%|█▌        | 51/339 [00:03<00:18, 15.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2979, 'grad_norm': 2.6805660724639893, 'learning_rate': 7.35e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 102/339 [00:06<00:13, 17.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6101, 'grad_norm': 1.4782524108886719, 'learning_rate': 0.00014849999999999998, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 152/339 [00:08<00:09, 18.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2788, 'grad_norm': 2.8734097480773926, 'learning_rate': 0.00022349999999999998, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 202/339 [00:11<00:06, 21.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.354, 'grad_norm': 3.0835440158843994, 'learning_rate': 0.0002985, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 254/339 [00:13<00:03, 22.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2819, 'grad_norm': 1.3314687013626099, 'learning_rate': 0.00019424460431654675, 'epoch': 2.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 302/339 [00:15<00:01, 19.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.154, 'grad_norm': 1.2306898832321167, 'learning_rate': 8.633093525179855e-05, 'epoch': 2.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 339/339 [00:17<00:00, 19.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 17.8337, 'train_samples_per_second': 152.072, 'train_steps_per_second': 19.009, 'train_loss': 2.454839003121255, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=339, training_loss=2.454839003121255, metrics={'train_runtime': 17.8337, 'train_samples_per_second': 152.072, 'train_steps_per_second': 19.009, 'total_flos': 25352295800832.0, 'train_loss': 2.454839003121255, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 5) Define & train model (no eval during training)\n",
    "# =========================\n",
    "import torch\n",
    "from transformers import GPT2Config, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "vocab_size = VOCAB_SIZE\n",
    "gpt_cfg = GPT2Config(\n",
    "    vocab_size=vocab_size,\n",
    "    n_positions=max(1024, final_len * 2),\n",
    "    n_embd=256,\n",
    "    n_layer=4,\n",
    "    n_head=8,\n",
    "    n_inner=1024,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_config(gpt_cfg)\n",
    "\n",
    "def collate(batch):\n",
    "    PAD = PAD_ID\n",
    "    maxlen = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    input_ids, labels, attn = [], [], []\n",
    "    for x in batch:\n",
    "        seq = x[\"input_ids\"]\n",
    "        pad = [PAD] * (maxlen - len(seq))\n",
    "        inp = seq + pad\n",
    "        lab = seq + pad\n",
    "        for j in range(len(seq), maxlen):\n",
    "            lab[j] = -100  # mask pads\n",
    "        input_ids.append(inp)\n",
    "        labels.append(lab)\n",
    "        attn.append([1]*len(seq) + [0]*len(pad))\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "    }\n",
    "\n",
    "BATCH = 8\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(WORK / \"hf_runs_simple\"),\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=200,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    # IMPORTANT: turn OFF eval to avoid numpy conversion in the eval loop\n",
    "    eval_strategy=\"no\",          # (use this new arg name on 4.44+)\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    report_to=[],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    optim=\"adamw_torch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    data_collator=collate,\n",
    "    # NOTE: don't pass eval_dataset here\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d054b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: c:\\Users\\rohit\\Downloads\\hacknc2025\\hacknc2025\\src\\ai\\nes_transformer\\samples\\sample_simple.mid\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 6) Generate continuation and write MIDI\n",
    "# =========================\n",
    "def tokens_to_midi(ids, out_path: Path, bpm: int = 140):\n",
    "    \"\"\"Decode simple tokens back to a single-track MIDI.\"\"\"\n",
    "    tpq = 480\n",
    "    ticks_per_step = tpq // 4  # 16th grid\n",
    "    cur_idx = 0\n",
    "    notes = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        tid = ids[i]\n",
    "        pitch = id_to_pitch(tid)\n",
    "        if pitch is None:\n",
    "            i += 1\n",
    "            continue\n",
    "        # start note\n",
    "        start = i\n",
    "        j = i + 1\n",
    "        while j < len(ids) and ids[j] == HOLD_ID:\n",
    "            j += 1\n",
    "        end = j\n",
    "        start_tick = start * ticks_per_step\n",
    "        end_tick   = max(start_tick + ticks_per_step, end * ticks_per_step)\n",
    "        notes.append((pitch, start_tick, end_tick))\n",
    "        i = j\n",
    "\n",
    "    # write MIDI\n",
    "    mf = MidiFile(ticks_per_beat=tpq)\n",
    "    inst = Instrument(program=80, is_drum=False, name=\"lead\")\n",
    "    inst.notes = [Note(velocity=90, pitch=p, start=s, end=e) for (p, s, e) in notes]\n",
    "    mf.instruments = [inst]\n",
    "    # Add a constant tempo (approximate)\n",
    "    from miditoolkit.midi.containers import TempoChange\n",
    "    mf.tempo_changes = [TempoChange(tempo=bpm, time=0)]\n",
    "    mf.dump(str(out_path))\n",
    "\n",
    "# Pick a short seed (or any)\n",
    "files = sorted(TOK_DIR.glob(\"*.json\"))\n",
    "assert files, \"No token files found—rerun Step 3.\"\n",
    "seed_path = min(files, key=lambda p: len(json.loads(p.read_text())[\"ids\"]))\n",
    "seed_ids  = json.loads(seed_path.read_text())[\"ids\"]\n",
    "\n",
    "inp = torch.tensor(seed_ids, dtype=torch.long)[None, :]\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\"); inp = inp.to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    gen = model.generate(\n",
    "        input_ids=inp,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=PAD_ID\n",
    "    )\n",
    "\n",
    "out_ids = gen[0].tolist()\n",
    "out_mid = SAMPLES / \"sample_simple.mid\"\n",
    "tokens_to_midi(out_ids, out_mid, bpm=140)\n",
    "print(\"Wrote:\", out_mid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nesxform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
