{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69405207",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train the GPT2 model on NES-MDB MIDI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d54c9eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]\n",
      "Torch: 2.4.0+cu121 | CUDA build: 12.1 | cuda available: True\n",
      "GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Transformers: 4.44.2\n",
      "Datasets: 2.21.0\n",
      "miditok: 3.0.6\n",
      "miditoolkit: 0.1.16\n",
      "pretty_midi: 0.2.9\n",
      "numpy: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # NES-MDB Chiptune Transformer — Minimal, Robust Notebook (2025)\n",
    "# - Tokenize raw MIDIs directly (monophonic skyline) with **MIDILike** (no durations)\n",
    "# - Build dataset with short-clip support and fallback seq_len\n",
    "# - Train a small GPT-like model (Transformers)\n",
    "# - Generate continuation + try to write MIDI\n",
    "#\n",
    "# Works with:\n",
    "#   torch (CUDA if available), transformers 4.4x, datasets 2.2x,\n",
    "#   miditok 3.0.6, miditoolkit 0.1.16, pretty_midi 0.2.9, numpy 2.x\n",
    "\n",
    "# =========================\n",
    "# 0) Compatibility & env check\n",
    "# =========================\n",
    "# %%\n",
    "import sys, os, json, random, numpy as np\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "\n",
    "# NumPy 2.x removed np.int / np.bool etc; some libs still reference them\n",
    "if not hasattr(np, \"int\"):    np.int = int\n",
    "if not hasattr(np, \"bool\"):   np.bool = bool\n",
    "if not hasattr(np, \"float\"):  np.float = float\n",
    "if not hasattr(np, \"object\"): np.object = object\n",
    "\n",
    "def pkgver(name: str) -> str:\n",
    "    try: return version(name)\n",
    "    except PackageNotFoundError: return \"not-found\"\n",
    "\n",
    "import torch\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA build:\", torch.version.cuda, \"| cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "import transformers, datasets\n",
    "print(\"Transformers:\", pkgver(\"transformers\"))\n",
    "print(\"Datasets:\",    pkgver(\"datasets\"))\n",
    "print(\"miditok:\",     pkgver(\"miditok\"))\n",
    "print(\"miditoolkit:\", pkgver(\"miditoolkit\"))\n",
    "print(\"pretty_midi:\", pkgver(\"pretty_midi\"))\n",
    "print(\"numpy:\",       np.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5828eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\rohit\\Downloads\\hacknc2025-1\\hacknc2025\\src\\ai\\data\\nesmdb_midi exists: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 1) Paths & knobs\n",
    "# =========================\n",
    "# %%\n",
    "from pathlib import Path\n",
    "\n",
    "REPO      = Path.cwd()\n",
    "DATA_DIR  = REPO / \"data\" / \"nesmdb_midi\"   # <--- put your raw MIDIs here\n",
    "WORK      = REPO / \"nes_transformer\"\n",
    "\n",
    "TOK_DIR   = WORK / \"tokens\"       # token JSONs { \"ids\": [...] }\n",
    "RUN_DIR   = WORK / \"hf_runs\"      # HF checkpoints / logs\n",
    "SAMPLES   = WORK / \"samples\"      # generated MIDIs\n",
    "for p in [WORK, TOK_DIR, RUN_DIR, SAMPLES]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR.resolve(), \"exists:\", DATA_DIR.exists())\n",
    "\n",
    "# --- tokenization speed/quality knobs ---\n",
    "SUBSET_N   = 2000      # process only this many raw MIDIs now (None = all)  << adjust for time\n",
    "SEED       = 42\n",
    "LO_PITCH   = 48        # C3\n",
    "HI_PITCH   = 96        # C7\n",
    "MAX_TICKS  = None      # e.g. 20000 to crop early for speed; None = full file (more robust)\n",
    "USE_THREADS= False     # start serial for stability; flip to True for speed once it works\n",
    "MAX_WORKERS= max(4, os.cpu_count() or 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7a98e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: MIDILike | vocab_size: 338\n",
      "Raw MIDIs found: 10556\n",
      "Using subset: 2000 / 10556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing (serial):   0%|          | 0/2000 [00:00<?, ?it/s]C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_50052\\2574420059.py:93: UserWarning: You are using a depreciated `miditoolkit.MidiFile` object. MidiTokis now (>v3.0.0) using symusic.Score as MIDI backend. Your file willbe converted on the fly, however please consider using symusic.\n",
      "  toks = tokenizer.tokenize(one) if hasattr(tokenizer, \"tokenize\") else tokenizer(one)\n",
      "Tokenizing (serial):   7%|▋         | 136/2000 [00:09<02:16, 13.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 114\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m tqdm(raw, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizing (serial)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 114\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m             token_files\u001b[38;5;241m.\u001b[39mappend(res)\n",
      "Cell \u001b[1;32mIn[9], line 66\u001b[0m, in \u001b[0;36mtokenize_one\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     mf \u001b[38;5;241m=\u001b[39m \u001b[43mMidiFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# collect notes (non-drum, in range), optional crop for speed\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     cand \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\miditoolkit\\midi\\parser.py:31\u001b[0m, in \u001b[0;36mMidiFile.__init__\u001b[1;34m(self, filename, file, ticks_per_beat, clip, charset)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;66;03m# filename\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m         mido_obj \u001b[38;5;241m=\u001b[39m \u001b[43mmido\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMidiFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcharset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m         mido_obj \u001b[38;5;241m=\u001b[39m mido\u001b[38;5;241m.\u001b[39mMidiFile(file\u001b[38;5;241m=\u001b[39mfile, clip\u001b[38;5;241m=\u001b[39mclip, charset\u001b[38;5;241m=\u001b[39mcharset)\n",
      "File \u001b[1;32mc:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\mido\\midifiles\\midifiles.py:320\u001b[0m, in \u001b[0;36mMidiFile.__init__\u001b[1;34m(self, filename, file, type, ticks_per_beat, charset, debug, clip, tracks)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m--> 320\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\mido\\midifiles\\midifiles.py:371\u001b[0m, in \u001b[0;36mMidiFile._load\u001b[1;34m(self, infile)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m    369\u001b[0m     _dbg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrack \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracks\u001b[38;5;241m.\u001b[39mappend(\u001b[43mread_track\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\mido\\midifiles\\midifiles.py:187\u001b[0m, in \u001b[0;36mread_track\u001b[1;34m(infile, debug, clip)\u001b[0m\n\u001b[0;32m    183\u001b[0m last_status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# End of track reached.\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43minfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtell\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m start \u001b[38;5;241m==\u001b[39m size:\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 2) Tokenize directly from RAW (MIDILike, skyline to 1-voice)\n",
    "# =========================\n",
    "# %%\n",
    "# Patch miditoolkit.Note with a .duration property (needed by miditok's converter under the hood)\n",
    "import miditoolkit\n",
    "try:\n",
    "    from miditoolkit.midi.containers import Note as MTKNote\n",
    "except Exception:\n",
    "    from miditoolkit import Note as MTKNote\n",
    "if not hasattr(MTKNote, \"duration\"):\n",
    "    MTKNote.duration = property(lambda self: self.end - self.start)\n",
    "\n",
    "from miditoolkit import MidiFile, Instrument, Note\n",
    "from miditok import MIDILike, TokenizerConfig, TokSequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Minimal tokenizer config (avoid durations/tempos/rets complexity)\n",
    "tok_config = TokenizerConfig(\n",
    "    beat_res={(0,0):4},     # 16th grid for positions\n",
    "    use_chords=False,\n",
    "    use_rests=False,\n",
    "    use_tempos=False,\n",
    "    use_time_signatures=False,\n",
    "    use_programs=False\n",
    ")\n",
    "tokenizer = MIDILike(tok_config)\n",
    "print(\"Tokenizer: MIDILike | vocab_size:\", tokenizer.vocab_size)\n",
    "\n",
    "# Skyline melody extractor (ticks): always monophonic\n",
    "def skyline_ticks(notes, min_dur=1):\n",
    "    if not notes: return []\n",
    "    times = sorted({n.start for n in notes} | {n.end for n in notes})\n",
    "    out = []\n",
    "    cur_pitch, cur_start = None, None\n",
    "    for t in times:\n",
    "        active = [n for n in notes if n.start <= t < n.end]\n",
    "        if active:\n",
    "            pitch = max(active, key=lambda n: n.pitch).pitch\n",
    "            if pitch != cur_pitch:\n",
    "                if cur_pitch is not None and (t - cur_start) >= min_dur:\n",
    "                    out.append(Note(velocity=90, pitch=cur_pitch, start=cur_start, end=t))\n",
    "                cur_pitch, cur_start = pitch, t\n",
    "        else:\n",
    "            if cur_pitch is not None and (t - cur_start) >= min_dur:\n",
    "                out.append(Note(velocity=90, pitch=cur_pitch, start=cur_start, end=t))\n",
    "            cur_pitch, cur_start = None, None\n",
    "    return out\n",
    "\n",
    "# Gather raw files (+ optional subset)\n",
    "raw_all = sorted([p for ext in (\"*.mid\",\"*.midi\",\"*.MID\",\"*.MIDI\") for p in DATA_DIR.rglob(ext)])\n",
    "print(\"Raw MIDIs found:\", len(raw_all))\n",
    "if SUBSET_N is not None and SUBSET_N < len(raw_all):\n",
    "    random.seed(SEED)\n",
    "    raw = sorted(random.sample(raw_all, SUBSET_N))\n",
    "    print(f\"Using subset: {len(raw)} / {len(raw_all)}\")\n",
    "else:\n",
    "    raw = raw_all\n",
    "    print(\"Using ALL raw files\")\n",
    "\n",
    "def tokenize_one(path: Path) -> Path | None:\n",
    "    out = TOK_DIR / (path.stem + \".json\")\n",
    "    if out.exists():\n",
    "        return out\n",
    "    try:\n",
    "        mf = MidiFile(str(path))\n",
    "        # collect notes (non-drum, in range), optional crop for speed\n",
    "        cand = []\n",
    "        for inst in mf.instruments:\n",
    "            if inst.is_drum or not inst.notes:\n",
    "                continue\n",
    "            notes = inst.notes\n",
    "            if MAX_TICKS is not None:\n",
    "                notes = [n for n in notes if n.start < MAX_TICKS]\n",
    "            notes = [n for n in notes if LO_PITCH <= n.pitch <= HI_PITCH]\n",
    "            cand.extend(notes)\n",
    "        if len(cand) < 4:\n",
    "            return None\n",
    "\n",
    "        mono = skyline_ticks(cand, min_dur=1)\n",
    "        if len(mono) < 4:\n",
    "            return None\n",
    "\n",
    "        # Build tiny 1-track MIDI in memory (keeps original grid)\n",
    "        one = MidiFile(ticks_per_beat=mf.ticks_per_beat)\n",
    "        one.tempo_changes = mf.tempo_changes\n",
    "        one.time_signature_changes = mf.time_signature_changes\n",
    "        inst = Instrument(program=80, is_drum=False, name=\"lead\")\n",
    "        inst.notes = mono\n",
    "        one.instruments = [inst]\n",
    "\n",
    "        # Tokenize (MIDILike)\n",
    "        toks = tokenizer.tokenize(one) if hasattr(tokenizer, \"tokenize\") else tokenizer(one)\n",
    "        ids  = toks.ids if hasattr(toks, \"ids\") else toks\n",
    "        if not ids:\n",
    "            return None\n",
    "\n",
    "        out.write_text(json.dumps({\"ids\": ids}))\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "token_files = []\n",
    "if USE_THREADS:\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futures = [ex.submit(tokenize_one, p) for p in raw]\n",
    "        for f in tqdm(as_completed(futures), total=len(futures), desc=\"Tokenizing (threads)\"):\n",
    "            res = f.result()\n",
    "            if res is not None:\n",
    "                token_files.append(res)\n",
    "else:\n",
    "    for p in tqdm(raw, desc=\"Tokenizing (serial)\"):\n",
    "        res = tokenize_one(p)\n",
    "        if res is not None:\n",
    "            token_files.append(res)\n",
    "\n",
    "print(\"Token files written:\", len(token_files), \"→\", TOK_DIR)\n",
    "\n",
    "# Simple peek\n",
    "if token_files:\n",
    "    sample_ids = json.loads(token_files[0].read_text())[\"ids\"]\n",
    "    print(\"Sample token length:\", len(sample_ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a3b0403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\rohit\\Downloads\\hacknc2025-1\\hacknc2025\\src\\ai\\data\\nesmdb_midi | exists: False\n",
      "Raw files: 0\n",
      "Using subset: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing (simple): 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token files written: 0 → c:\\Users\\rohit\\Downloads\\hacknc2025-1\\hacknc2025\\src\\ai\\nes_transformer\\tokens_simple\n",
      "VOCAB_SIZE: 131 | PAD/REST/HOLD ids: 0 1 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 3) Simple 16th-grid tokenizer (no MidiTok)\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "import json, os, random, math\n",
    "\n",
    "from miditoolkit import MidiFile, Instrument, Note\n",
    "\n",
    "# ---- Paths (redefine if kernel was restarted) ----\n",
    "REPO      = Path.cwd()\n",
    "DATA_DIR  = REPO / \"data\" / \"nesmdb_midi\"   # raw MIDIs here\n",
    "WORK      = REPO / \"nes_transformer\"\n",
    "TOK_DIR   = WORK / \"tokens_simple\"\n",
    "SAMPLES   = WORK / \"samples\"\n",
    "for p in [WORK, TOK_DIR, SAMPLES]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR.resolve(), \"| exists:\", DATA_DIR.exists())\n",
    "\n",
    "# ---- Token vocabulary: PAD=0, REST=1, HOLD=2, P0..P127=3..130 ----\n",
    "PAD_ID  = 0\n",
    "REST_ID = 1\n",
    "HOLD_ID = 2\n",
    "PITCH_BASE = 3  # P0 maps to 3, P127 maps to 130\n",
    "VOCAB_SIZE = PITCH_BASE + 128\n",
    "\n",
    "def pitch_to_id(p: int) -> int:\n",
    "    p = max(0, min(127, int(p)))\n",
    "    return PITCH_BASE + p\n",
    "\n",
    "def id_to_pitch(i: int) -> int | None:\n",
    "    if i >= PITCH_BASE and i < PITCH_BASE + 128:\n",
    "        return i - PITCH_BASE\n",
    "    return None  # REST/HOLD/PAD\n",
    "\n",
    "# ---- Monophonic skyline over ticks ----\n",
    "def skyline_ticks(notes, min_dur_ticks: int = 1):\n",
    "    if not notes:\n",
    "        return []\n",
    "    times = sorted({n.start for n in notes} | {n.end for n in notes})\n",
    "    out = []\n",
    "    cur_pitch, cur_start = None, None\n",
    "    for t in times:\n",
    "        active = [n for n in notes if n.start <= t < n.end]\n",
    "        if active:\n",
    "            pitch = max(active, key=lambda n: n.pitch).pitch\n",
    "            if pitch != cur_pitch:\n",
    "                if cur_pitch is not None and (t - cur_start) >= min_dur_ticks:\n",
    "                    out.append((cur_pitch, cur_start, t))\n",
    "                cur_pitch, cur_start = pitch, t\n",
    "        else:\n",
    "            if cur_pitch is not None and (t - cur_start) >= min_dur_ticks:\n",
    "                out.append((cur_pitch, cur_start, t))\n",
    "            cur_pitch, cur_start = None, None\n",
    "    return out\n",
    "\n",
    "# ---- Quantize monophonic notes to a 16th grid ----\n",
    "def quantize_to_grid_16th(mf: MidiFile, mono_notes, max_steps: int | None = None):\n",
    "    tpq = max(1, int(mf.ticks_per_beat))\n",
    "    ticks_per_step = max(1, tpq // 4)  # 16th = TPQ/4\n",
    "    if not mono_notes:\n",
    "        return []\n",
    "\n",
    "    max_tick = 0\n",
    "    for pitch, s, e in mono_notes:\n",
    "        max_tick = max(max_tick, e)\n",
    "\n",
    "    total_steps = math.ceil(max_tick / ticks_per_step)\n",
    "    if max_steps is not None:\n",
    "        total_steps = min(total_steps, max_steps)\n",
    "\n",
    "    seq = [REST_ID] * max(1, total_steps)\n",
    "\n",
    "    cur_idx = 0\n",
    "    for pitch, s, e in mono_notes:\n",
    "        start_idx = int(round(s / ticks_per_step))\n",
    "        end_idx   = int(max(start_idx + 1, round(e / ticks_per_step)))\n",
    "        if max_steps is not None:\n",
    "            start_idx = min(start_idx, max_steps - 1)\n",
    "            end_idx   = min(end_idx,   max_steps)\n",
    "\n",
    "        # fill leading rest if any\n",
    "        while cur_idx < start_idx and cur_idx < len(seq):\n",
    "            seq[cur_idx] = REST_ID\n",
    "            cur_idx += 1\n",
    "\n",
    "        if start_idx < len(seq):\n",
    "            seq[start_idx] = pitch_to_id(pitch)\n",
    "            cur_idx = start_idx + 1\n",
    "\n",
    "        # fill holds\n",
    "        while cur_idx < end_idx and cur_idx < len(seq):\n",
    "            seq[cur_idx] = HOLD_ID\n",
    "            cur_idx += 1\n",
    "\n",
    "    return seq\n",
    "\n",
    "# ---- File → tokens pipeline ----\n",
    "def midi_to_token_ids(path: Path, lo_pitch: int | None = None, hi_pitch: int | None = None,\n",
    "                      crop_ticks: int | None = None, max_steps: int | None = 2048):\n",
    "    try:\n",
    "        mf = MidiFile(str(path))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    # collect non-drum notes (optionally filter pitch / crop)\n",
    "    notes = []\n",
    "    for inst in mf.instruments:\n",
    "        if inst.is_drum or not inst.notes:\n",
    "            continue\n",
    "        ns = inst.notes\n",
    "        if crop_ticks is not None:\n",
    "            ns = [n for n in ns if n.start < crop_ticks]\n",
    "        if lo_pitch is not None and hi_pitch is not None:\n",
    "            ns = [n for n in ns if lo_pitch <= n.pitch <= hi_pitch]\n",
    "        notes.extend(ns)\n",
    "\n",
    "    if not notes:\n",
    "        return None\n",
    "\n",
    "    mono = skyline_ticks(notes, min_dur_ticks=1)\n",
    "    if not mono:\n",
    "        return None\n",
    "\n",
    "    ids = quantize_to_grid_16th(mf, mono, max_steps=max_steps)\n",
    "    # keep only reasonably sized sequences\n",
    "    return ids if len(ids) >= 16 else None\n",
    "\n",
    "# ---- Batch tokenization ----\n",
    "SUBSET_N   = 2000          # start small to confirm; bump to 1500–3000 when it works\n",
    "SEED       = 42\n",
    "LO_PITCH   = None         # None means keep all pitches (widest, safest)\n",
    "HI_PITCH   = None\n",
    "CROP_TICKS = None         # set e.g. 20000 to speed up\n",
    "MAX_STEPS  = 1024         # cap per piece\n",
    "\n",
    "raw_all = sorted([p for ext in (\"*.mid\",\"*.midi\",\"*.MID\",\"*.MIDI\") for p in DATA_DIR.rglob(ext)])\n",
    "print(\"Raw files:\", len(raw_all))\n",
    "random.seed(SEED)\n",
    "raw = sorted(random.sample(raw_all, min(SUBSET_N, len(raw_all))))\n",
    "print(\"Using subset:\", len(raw))\n",
    "\n",
    "written = 0\n",
    "for p in tqdm(raw, desc=\"Tokenizing (simple)\"):\n",
    "    out = TOK_DIR / f\"{p.stem}.json\"\n",
    "    if out.exists():\n",
    "        written += 1\n",
    "        continue\n",
    "    ids = midi_to_token_ids(p, lo_pitch=LO_PITCH, hi_pitch=HI_PITCH,\n",
    "                            crop_ticks=CROP_TICKS, max_steps=MAX_STEPS)\n",
    "    if ids is None:\n",
    "        continue\n",
    "    out.write_text(json.dumps({\"ids\": ids}))\n",
    "    written += 1\n",
    "\n",
    "print(\"Token files written:\", written, \"→\", TOK_DIR)\n",
    "print(\"VOCAB_SIZE:\", VOCAB_SIZE, \"| PAD/REST/HOLD ids:\", PAD_ID, REST_ID, HOLD_ID)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef835d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ_LEN=512 -> sequences: 0\n",
      "SEQ_LEN=256 -> sequences: 0\n",
      "SEQ_LEN=128 -> sequences: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No sequences produced. Increase SUBSET_N, set CROP_TICKS=None, or lower keep_short_min to 4, then re-run.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_sequences \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo sequences produced. Increase SUBSET_N, set CROP_TICKS=None, or lower keep_short_min to 4, then re-run.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(final_sequences)\n\u001b[0;32m     41\u001b[0m ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_list(final_sequences)\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No sequences produced. Increase SUBSET_N, set CROP_TICKS=None, or lower keep_short_min to 4, then re-run."
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 4) Build dataset (keeps short clips; 512→256→128 fallback)\n",
    "# =========================\n",
    "from datasets import Dataset\n",
    "import json, random\n",
    "\n",
    "def build_ds(token_dir: Path, seq_len: int, keep_short_min: int = 8, step_frac: float = 0.5):\n",
    "    files = sorted(token_dir.glob(\"*.json\"))\n",
    "    sequences = []\n",
    "    step = max(1, int(seq_len * step_frac))\n",
    "    for p in files:\n",
    "        try:\n",
    "            ids = json.loads(p.read_text())[\"ids\"]\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not ids:\n",
    "            continue\n",
    "        if len(ids) <= seq_len:\n",
    "            if len(ids) >= keep_short_min:\n",
    "                sequences.append({\"input_ids\": ids, \"labels\": ids.copy()})\n",
    "            continue\n",
    "        # sliding window\n",
    "        for i in range(0, len(ids) - seq_len + 1, step):\n",
    "            seq = ids[i:i+seq_len]\n",
    "            sequences.append({\"input_ids\": seq, \"labels\": seq.copy()})\n",
    "    return sequences\n",
    "\n",
    "SEQ_TRY = [512, 256, 128]\n",
    "final_sequences, final_len = None, None\n",
    "for L in SEQ_TRY:\n",
    "    seqs = build_ds(TOK_DIR, seq_len=L, keep_short_min=8, step_frac=0.5)\n",
    "    print(f\"SEQ_LEN={L} -> sequences: {len(seqs)}\")\n",
    "    if seqs:\n",
    "        final_sequences, final_len = seqs, L\n",
    "        break\n",
    "\n",
    "if final_sequences is None:\n",
    "    raise RuntimeError(\"No sequences produced. Increase SUBSET_N, set CROP_TICKS=None, or lower keep_short_min to 4, then re-run.\")\n",
    "\n",
    "random.shuffle(final_sequences)\n",
    "ds = Dataset.from_list(final_sequences).train_test_split(test_size=0.05, seed=42)\n",
    "print(f\"USING SEQ_LEN={final_len} | train={len(ds['train'])} | test={len(ds['test'])}\")\n",
    "ds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9569fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "  0%|          | 0/339 [00:00<?, ?it/s]c:\\Users\\rohit\\.conda\\envs\\nesxform\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:544: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      " 15%|█▌        | 51/339 [00:03<00:18, 15.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2979, 'grad_norm': 2.6805660724639893, 'learning_rate': 7.35e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 102/339 [00:06<00:13, 17.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6101, 'grad_norm': 1.4782524108886719, 'learning_rate': 0.00014849999999999998, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 152/339 [00:08<00:09, 18.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2788, 'grad_norm': 2.8734097480773926, 'learning_rate': 0.00022349999999999998, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 202/339 [00:11<00:06, 21.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.354, 'grad_norm': 3.0835440158843994, 'learning_rate': 0.0002985, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 254/339 [00:13<00:03, 22.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2819, 'grad_norm': 1.3314687013626099, 'learning_rate': 0.00019424460431654675, 'epoch': 2.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 302/339 [00:15<00:01, 19.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.154, 'grad_norm': 1.2306898832321167, 'learning_rate': 8.633093525179855e-05, 'epoch': 2.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 339/339 [00:17<00:00, 19.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 17.8337, 'train_samples_per_second': 152.072, 'train_steps_per_second': 19.009, 'train_loss': 2.454839003121255, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=339, training_loss=2.454839003121255, metrics={'train_runtime': 17.8337, 'train_samples_per_second': 152.072, 'train_steps_per_second': 19.009, 'total_flos': 25352295800832.0, 'train_loss': 2.454839003121255, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 5) Define & train model (no eval during training)\n",
    "# =========================\n",
    "import torch\n",
    "from transformers import GPT2Config, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "vocab_size = VOCAB_SIZE\n",
    "gpt_cfg = GPT2Config(\n",
    "    vocab_size=vocab_size,\n",
    "    n_positions=max(1024, final_len * 2),\n",
    "    n_embd=256,\n",
    "    n_layer=4,\n",
    "    n_head=8,\n",
    "    n_inner=1024,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_config(gpt_cfg)\n",
    "\n",
    "def collate(batch):\n",
    "    PAD = PAD_ID\n",
    "    maxlen = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    input_ids, labels, attn = [], [], []\n",
    "    for x in batch:\n",
    "        seq = x[\"input_ids\"]\n",
    "        pad = [PAD] * (maxlen - len(seq))\n",
    "        inp = seq + pad\n",
    "        lab = seq + pad\n",
    "        for j in range(len(seq), maxlen):\n",
    "            lab[j] = -100  # mask pads\n",
    "        input_ids.append(inp)\n",
    "        labels.append(lab)\n",
    "        attn.append([1]*len(seq) + [0]*len(pad))\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "    }\n",
    "\n",
    "BATCH = 8\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(WORK / \"hf_runs_simple\"),\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=200,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    # IMPORTANT: turn OFF eval to avoid numpy conversion in the eval loop\n",
    "    eval_strategy=\"no\",          # (use this new arg name on 4.44+)\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    report_to=[],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    optim=\"adamw_torch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    data_collator=collate,\n",
    "    # NOTE: don't pass eval_dataset here\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d054b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: c:\\Users\\rohit\\Downloads\\hacknc2025\\hacknc2025\\src\\ai\\nes_transformer\\samples\\sample_simple.mid\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 6) Generate continuation and write MIDI\n",
    "# =========================\n",
    "def tokens_to_midi(ids, out_path: Path, bpm: int = 140):\n",
    "    \"\"\"Decode simple tokens back to a single-track MIDI.\"\"\"\n",
    "    tpq = 480\n",
    "    ticks_per_step = tpq // 4  # 16th grid\n",
    "    cur_idx = 0\n",
    "    notes = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        tid = ids[i]\n",
    "        pitch = id_to_pitch(tid)\n",
    "        if pitch is None:\n",
    "            i += 1\n",
    "            continue\n",
    "        # start note\n",
    "        start = i\n",
    "        j = i + 1\n",
    "        while j < len(ids) and ids[j] == HOLD_ID:\n",
    "            j += 1\n",
    "        end = j\n",
    "        start_tick = start * ticks_per_step\n",
    "        end_tick   = max(start_tick + ticks_per_step, end * ticks_per_step)\n",
    "        notes.append((pitch, start_tick, end_tick))\n",
    "        i = j\n",
    "\n",
    "    # write MIDI\n",
    "    mf = MidiFile(ticks_per_beat=tpq)\n",
    "    inst = Instrument(program=80, is_drum=False, name=\"lead\")\n",
    "    inst.notes = [Note(velocity=90, pitch=p, start=s, end=e) for (p, s, e) in notes]\n",
    "    mf.instruments = [inst]\n",
    "    # Add a constant tempo (approximate)\n",
    "    from miditoolkit.midi.containers import TempoChange\n",
    "    mf.tempo_changes = [TempoChange(tempo=bpm, time=0)]\n",
    "    mf.dump(str(out_path))\n",
    "\n",
    "# Pick a short seed (or any)\n",
    "files = sorted(TOK_DIR.glob(\"*.json\"))\n",
    "assert files, \"No token files found—rerun Step 3.\"\n",
    "seed_path = min(files, key=lambda p: len(json.loads(p.read_text())[\"ids\"]))\n",
    "seed_ids  = json.loads(seed_path.read_text())[\"ids\"]\n",
    "\n",
    "inp = torch.tensor(seed_ids, dtype=torch.long)[None, :]\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\"); inp = inp.to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    gen = model.generate(\n",
    "        input_ids=inp,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=PAD_ID\n",
    "    )\n",
    "\n",
    "out_ids = gen[0].tolist()\n",
    "out_mid = SAMPLES / \"sample_simple.mid\"\n",
    "tokens_to_midi(out_ids, out_mid, bpm=140)\n",
    "print(\"Wrote:\", out_mid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a892aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these cells at the end of train.ipynb AFTER trainer.train()\n",
    "\n",
    "# Save HF format for the Python server\n",
    "model_dir = WORK / \"model_export\"\n",
    "model.save_pretrained(model_dir)\n",
    "print(\"Saved model to\", model_dir)\n",
    "\n",
    "# (Optional) Export ONNX for Node runtime\n",
    "import torch\n",
    "onnx_path = WORK / \"model.onnx\"\n",
    "dummy = torch.randint(0, PITCH_BASE+128, (1, 32), dtype=torch.long)\n",
    "attn = torch.ones_like(dummy)\n",
    "torch.onnx.export(\n",
    "    model, (dummy, attn),\n",
    "    str(onnx_path),\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\"input_ids\": {0: \"batch\", 1: \"seq\"}, \"attention_mask\": {0:\"batch\", 1:\"seq\"}, \"logits\": {0:\"batch\", 1:\"seq\"}},\n",
    "    opset_version=17\n",
    ")\n",
    "print(\"Wrote\", onnx_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nesxform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
